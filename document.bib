% This file was created with JabRef 2.10b2.
% Encoding: UTF-8


@Misc{tensorflow2015-whitepaper,
  Title                    = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},

  Author                   = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dan~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  Note                     = {Software available from tensorflow.org},
  Year                     = {2015},

  Url                      = {\url{http://tensorflow.org/}}
}

@Article{toxin-nn,
  Title                    = {Classification of toxin-induced changes in 1H NMR spectra of urine using an artificial neural network},
  Author                   = {Anthony, M. and Rose, V. and Nicholson, J. and Lindon, J.},
  Journal                  = {Journal of Pharmaceutical and Biomedical Analysis},
  Year                     = {1995},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {205-211},
  Volume                   = {13},

  Abstract                 = {NMR spectra of urine from rats treated with a range of liver, kidney and testicular toxins at various doses were measured and classified using neural network methods. Toxin-induced changes in the levels of 18 low molecular weight endogenous urinary metabolites were assessed using a simple semi-quantitative scoring system. These scores were used as input to an artificial neural network, the use of which has been explored as a means of predicting the class of toxin. With this limited data set, based only the level of the maximal changes of these 18 metabolites, the network was able to predict the class and hence target organ of the toxins. Renal cortical toxicity was well predicted as was liver toxicity. The few examples of renal medullary toxins in the data set resulted in relatively poor training of the network although correct classification was still possible.},
  Comment                  = {Use of feedforward neural networks to predict presence of toxins based on pre-processed metabolite changes. Similar sort of goal, and decent success rate, but requires a large degree of pre-processing and only identifies categories of toxins.},
  Keywords                 = {1H NMR; pattern recognition; urine; neural net; toxicity},
  Owner                    = {posul},
  Timestamp                = {2016.08.02},
  Url                      = {\url{http://www.sciencedirect.com/science/article/pii/073170859501278S}}
}

@Conference{bp-ga,
  Title                    = {Combining Back-Propagation and Genetic Algorithms to Train Neural Networks for Ambient Temperature Modeling in Italy},
  Author                   = {Ceravolo, F. and De Felice, M. and Pizzuti, S.},
  Booktitle                = {Applications of Evolutionary Computing},
  Year                     = {2009},

  Abstract                 = {This paper presents a hybrid approach based on soft computing techniques in order to estimate ambient temperature for those places where such datum is not available. Indeed, we combine the Back-Propagation (BP) algorithm and the Simple Genetic Algorithm (GA) in order to effectively train neural networks in such a way that the BP algorithm initialises a few individuals of the GAâ€™s population. Experiments have been performed over all the available Italian places and results have shown a remarkable improvement in accuracy compared to the single and traditional methods.},
  Comment                  = {Details a technique allowing for the hybridization of Backpropagation and Genetic Algorithm approaches, and provides a case study where it is used to successfully train a generative temperature model.},
  Keywords                 = {Neural Networks, Back-Propagation Algorithm, Simple Genetic Algorithm, Ambient Temperature Modeling, Sustainable Building Design},
  Owner                    = {posul},
  Timestamp                = {2016.08.03},
  Url                      = {\url{http://link.springer.com/chapter/10.1007%2F978-3-642-01129-0_16#page-1}}
}

@Article{nn-2dnmr,
  Title                    = {An Artificial Neural Network for Classifiying Cross Peaks in Two-Dimensional NMR Spectra},
  Author                   = {Corne, S. and Johnson, A. and Fisher, J.},
  Journal                  = {Journal of Magnetic Resonance},
  Year                     = {1992},

  Month                    = {November},
  Number                   = {2},
  Pages                    = {256-266},
  Volume                   = {100},

  Abstract                 = {A simulated neural network is described that has been trained to classify cross peaks in the 2D NMR spectra of biological macromolecules. The trained network has then been used to classify previously unseen data. The network is able to distinguish between authentic cross peaks and spectral artifacts, such as those arising from presaturation of water, noise, and t1 noise. Moreover, the network is able to recognize genuine peaks whose shapes have been modified, for example, by overlap with other real or spurious peaks. Herein, the training and performance of the network are demonstrated for a NOESY spectrum.},
  Comment                  = {Small-scale 121-8-2 neural net trained to classify spurious and genuine peaks in 2-dimensional data provided by STELLA. Overall accuracy ~86%. No compound labeling of resulting peaks.},
  Owner                    = {posul},
  Timestamp                = {2016.08.03},
  Url                      = {\url{http://www.sciencedirect.com/science/article/pii/002223649290260E}}
}

@Conference{dblstm1,
  Title                    = {Hybrid Speech Recognition With Deep Bidirectional LSTM},
  Author                   = {Graves, A. and Jaitly, N. and Mohamed, A.},
  Booktitle                = {2013 IEEE Automatic Speech Recognition and Understanding},
  Year                     = {2013},
  Month                    = {December},
  Pages                    = {273-278},
  Publisher                = {IEEE},

  Abstract                 = {Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We find that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.},
  Comment                  = {Theoretical introduction to the DBLSTM architecture, as well as a case study demonstrating its superiority to standard LSTM models for spech recognition.},
  Keywords                 = {DBLSTM, HMM-RNN hybrid},
  Owner                    = {posul},
  Timestamp                = {2016.08.02},
  Url                      = {\url{http://www.cs.toronto.edu/~graves/asru_2013.pdf}}
}

@Article{lstm,
  Title                    = {Long Short-Term Memory},
  Author                   = {Hochreiter, S. and Schmidhuber, J.},
  Journal                  = {Neural Computation},
  Year                     = {1997},

  Month                    = {November},
  Number                   = {8},
  Pages                    = {1735-1780},
  Volume                   = {9},

  Abstract                 = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  Comment                  = {The seminal paper introducing LSTM networks.},
  Owner                    = {posul},
  Timestamp                = {2016.08.03},
  Url                      = {\url{http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf}}
}

@Conference{xavier-weights,
  Title                    = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  Author                   = {Kaiming, H. and Xiangyu, Z. and Shaoqing, R. and Jian, S.},
  Booktitle                = {IEEE International Conference on Computer Vision},
  Year                     = {2015},
  Month                    = {December},
  Pages                    = {1026-1034},

  Abstract                 = {Rectified activation units (rectifiers) are essential for
state-of-the-art neural networks. In this work, we study
rectifier neural networks for image classification from two
aspects. First, we propose a Parametric Rectified Linear
Unit (PReLU) that generalizes the traditional rectified unit.
PReLU improves model fitting with nearly zero extra computational
cost and little overfitting risk. Second, we derive
a robust initialization method that particularly considers
the rectifier nonlinearities. This method enables us to
train extremely deep rectified models directly from scratch
and to investigate deeper or wider network architectures.
Based on our PReLU networks (PReLU-nets), we achieve
4.94% top-5 test error on the ImageNet 2012 classification
dataset. This is a 26% relative improvement over the
ILSVRC 2014 winner (GoogLeNet, 6.66% [29]). To our
knowledge, our result is the first to surpass human-level performance
(5.1%, [22]) on this visual recognition challenge.},
  Comment                  = {Provides rational for our DBLSTM weight initialization. Enables faster convergence of deep networks depending on Rectified Linear Units (ReLUs).},
  Owner                    = {posul},
  Timestamp                = {2016.08.03},
  Url                      = {\url{https://arxiv.org/pdf/1502.01852v1.pdf}}
}

@Conference{lstm-dropout,
  Title                    = {Dropout improves Recurrent Neural Networks for Handwriting Recognition},
  Author                   = {Pham, V. and Bluche, T. and Kermorvant, C. and Louradour, J.},
  Booktitle                = {International Conference on Frontiers in Handwriting Recognition},
  Year                     = {2014},
  Month                    = {September},
  Organization             = {IEEE},
  Pages                    = {285-290},
  Volume                   = {14},

  Comment                  = {Introduces the benefits and methodology of implementing dropout in LSTM networks. Justification and model for our own LSTM dropout.},
  Owner                    = {posul},
  Timestamp                = {2016.08.03},
  Url                      = {\url{https://arxiv.org/pdf/1312.4569.pdf}}
}

@Conference{ga-self-configured,
  Title                    = {Self-Configuring Genetic Algorithm with Modified Uniform Crossover Operator},
  Author                   = {Semenkin, E. and Semenkina, M.},
  Booktitle                = {WCCI 2012 World Congress on Computational Intelligence},
  Year                     = {2012},
  Month                    = {June},
  Organization             = {IEEE},
  Pages                    = {414-421},

  Abstract                 = {For genetic programming algorithms new variants of uniform crossover operators that introduce selective pressure on the recombination stage are proposed. Operators probabilistic rates based approach to GP self-configuration is suggested. Proposed modifications usefulness is demonstrated on benchmark test and real world problems.},
  Comment                  = {Basis for the self-configuring GA we supplement our backpropagation with. Introduces a uniform crossover operator which learns its hyperparameters during a run.},
  Keywords                 = {Genetic Algorithms, Uniform Crossover, Selective Pressure Recombination, Self-Configuration, Performance Comparison},
  Owner                    = {posul},
  Timestamp                = {2016.08.03},
  Url                      = {\url{http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=6256587}}
}

@Conference{dlstm-archi,
  Title                    = {Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition},
  Author                   = {Xiangang, L. and Xihong, W.},
  Booktitle                = {International Conference on Acoustics, Speech and Signal Processing},
  Year                     = {2015},
  Month                    = {April},
  Organization             = {IEEE},

  Comment                  = {Experiments with a range of deep LSTM architectures. Justification for our use of projection layers and stacked feedforward ReLU layers in our own DBLSTM.},
  Owner                    = {posul},
  Timestamp                = {2016.08.03},
  Url                      = {\url{https://arxiv.org/pdf/1410.4281.pdf}}
}

